> source('main2.r', echo = T)

> rm(list=ls())

> source('loadAndProcess2.R')

> source('trainLinearReg.R')

> source('sigmoid.R')

> source('costFunctionReg.R')

> source('gradFunctionReg.R')

> source('predict.R')

> source('learningCurveNN.R')

> source('trainNN.R')

> source('nnCostFunction.R')

> source('nnGradFunction.R')

> source('sigmoidGradient.R')

> source('nnPredict.R')

> source('randInitWeights.R')

> source('checkNNGradients.R')

> source('debugInitializeWeights.R')

> source('computeNumericalGradient.R')

> source('myfunctions2.R')

> library(optimx)

> ##############################
> #load and preprocess training#
> # validation, and test data    #
> ##############################
> dataset <- loadA .... [TRUNCATED] 
[1] "Loading Dataset....."

> X <- dataset[[1]]

> y <- dataset[[2]]

> Xval <- dataset[[3]]

> yval <- dataset[[4]]

> Xtest <- dataset[[5]]

> ytest <- dataset[[6]]

> m <- dim(X)[1]

> n <- dim(X)[2]

> # reduce the size of training set
> X <- X[0:m,]

> y <- y[0:m]

> hidden_layer_size <- 2

> lambda <- 0

> ##############################
> # gradient checking   ##########
> # disable it when training ###
> ##############################
> #checkNNGradient .... [TRUNCATED] 

> result<-trainNN(hidSize, X, y, lambda)
[1] "Begining Training Neural Networks"
[1] "the length of weights 517"

> #[nn] Obtain Theta1 and Theta2 back from nn_params
> nn_params <- t(coef(result))

> print(sprintf('The time taken for training: %f', result$xtimes))
[1] "The time taken for training: 2852.430000"

> print(sprintf('The training error cost: %f', result$value))
[1] "The training error cost: 0.123189"

> valcost <- nnCostFunction(nn_params, hidSize, Xval, yval, lambda)

> testcost <- nnCostFunction(nn_params, hidSize, Xtest, ytest, lambda)

> print(sprintf('The validation error cost: %f', valcost))
[1] "The validation error cost: 0.356736"

> print(sprintf('The testing error cost: %f', testcost))
[1] "The testing error cost: 0.433588"

> #print(result$convcode)
> n1 <- n * hidSize

> Theta1 <- matrix(nn_params[1:n1], nrow=hidSize, ncol=n)

> n2 <- length(nn_params)

> Theta2 <- matrix(nn_params[n1+1:n2], nrow=1, ncol=hidSize+1)

> pred <- nnPredict(Theta1, Theta2, X)

> print(sprintf('Training Accuracy: %f', mean((pred == y)) * 100))
[1] "Training Accuracy: 95.646000"

> pred <- nnPredict(Theta1, Theta2, Xval)

> print(sprintf('Validation Accuracy: %f', mean((pred == yval)) * 100))
[1] "Validation Accuracy: 88.033000"

> pred <- nnPredict(Theta1, Theta2, Xtest)

> print(sprintf('Testing Accuracy: %f', mean((pred == ytest)) * 100))
[1] "Testing Accuracy: 84.026000"

> ##############################
> ###### learning curve ########
> ##############################
> #valres <- learningCurveNN(hidSize, X=X, y=y, Xva .... [TRUNCATED] 
There were 50 or more warnings (use warnings() to see the first 50)
